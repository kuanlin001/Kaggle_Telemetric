{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>W207 Final Project</h3>\n",
    "\n",
    "Team Pacific Knights<br/>\n",
    "Members: Alan Wang, Daniel Sheinin, Kuan Lin, Michael Andrew Kennedy, Saru Mehta\n",
    "\n",
    "Competition Description:<br/>\n",
    "“What’s Cooking” - https://www.kaggle.com/c/whats-cooking\n",
    "The goal of this competition is to successfully classify a set of recipes into one of twenty geographic regions of origin according to the ingredients they use. The competition provides a labeled training data set containing lists of raw ingredients and the cuisine they belong to. A second unlabeled data set is provided for scoring purposes.  Competitors are ranked by their ability to accurately label the test set. Competitors are granted up to 5 scoring attempts per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = os.path.join(\".\",\"train.json\",\"train.json\")\n",
    "with open(train_file) as data_file:\n",
    "    train_data = json.loads(data_file.read())\n",
    "\n",
    "# using 1/3 of the train data for dev testing\n",
    "dev_test_data = ['\\n'.join(d[\"ingredients\"]) for d in train_data[:len(train_data)/3]]\n",
    "dev_test_label = [d[\"cuisine\"] for d in train_data[:len(train_data)/3]]\n",
    "dev_train_data = ['\\n'.join(d[\"ingredients\"]) for d in train_data[len(train_data)/3:]]\n",
    "dev_train_label = [d[\"cuisine\"] for d in train_data[len(train_data)/3:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Model:<br/>\n",
    "<ul>\n",
    "<li>Text model with TD-IDF vectorizer</li>\n",
    "<li>ngram range (1,2) with maximum document frequency set to 0.5</li>\n",
    "<li>Text preprocessore: lower all cases and replace \"-\" and \"_\" and they are commonly use to deliminate recipes</li>\n",
    "<li>Grid Search logistic regression and Multinomial NB</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizing texts...\n",
      "grid search on logistic regression:\n",
      "best parameters:\n",
      "{'penalty': 'l2', 'C': 11.0}\n",
      "accuracy: 0.7859"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\grid_search.py:370: ChangedBehaviorWarning: The long-standing behavior to use the estimator's score function in GridSearchCV.score has changed. The scoring parameter is now used.\n",
      "  ChangedBehaviorWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:607: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = (np.log(smoothed_fc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "grid search on MultinomialNB:\n",
      "best parameters:\n",
      "{'alpha': 0.01}\n",
      "accuracy: 0.7385\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "def text_preprocessor(s):\n",
    "    return s.lower().replace(\"-\", \" \").replace(\"_\", \" \")\n",
    "\n",
    "print \"vectorizing texts...\"\n",
    "vec = TfidfVectorizer(preprocessor=text_preprocessor, ngram_range=(1,2), max_df=0.5, strip_accents='unicode')\n",
    "dev_train_vec = vec.fit_transform(dev_train_data)\n",
    "dev_test_vec = vec.transform(dev_test_data)\n",
    "\n",
    "print \"grid search on logistic regression:\"\n",
    "params = {'C': [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 1.5, 2.0, 5.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0], 'penalty': ['l1', 'l2']}\n",
    "model_logistic = GridSearchCV(LogisticRegression(), param_grid=params, scoring='accuracy')\n",
    "model_logistic.fit(dev_train_vec, dev_train_label)\n",
    "print \"best parameters:\"\n",
    "print str(model_logistic.best_params_)\n",
    "print \"accuracy: %.4f\" % model_logistic.score(dev_test_vec, dev_test_label)\n",
    "print\n",
    "print \"grid search on MultinomialNB:\"\n",
    "alphas = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0]}\n",
    "model_MNB = GridSearchCV(MultinomialNB(), param_grid=alphas)\n",
    "model_MNB.fit(dev_train_vec, dev_train_label)\n",
    "print \"best parameters:\"\n",
    "print str(model_MNB.best_params_)\n",
    "print \"accuracy: %.4f\" % model_MNB.score(dev_test_vec, dev_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best initial attempt is logistic regression with L2 penalty and C regularization at 11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error Analysis #1:\n",
    "<ul>\n",
    "<li>Use confusion matrix to identify common errors</li>\n",
    "<li>Look at which recipes are causing the confusion</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make a confusion matrix on the best initial model\n",
      "  84    0    2    0    3    6    0    4    0   13    0    0    0   23    0    2   16    5    6    0 \n",
      "   0  105    3    1    1   44    3    7   19   21    5    1    0    6    0    4   56    1    1    0 \n",
      "   1    2  379    0    1   29    0    2    1   27    1    0    0   18    1    4   73    2    0    0 \n",
      "   1    3    3  769    6    2    0    7    0   14    0   15   16    4    0    2   17    1   23   12 \n",
      "   2    3    1   31  153    6    0    2    0   10    1    2    1   11    1    0   14    2    3    6 \n",
      "   0    4    6    1    3  565    6    7    8  166    0    2    0    8    2    8   65   16    0    1 \n",
      "   0    0    0    2    0    9  272    9    1   65    0    1    0    3   12    1    4    8    0    0 \n",
      "   2    0    0    2    3    4    9  882    0    6    1    0    1   21   15    0   11    0    8    0 \n",
      "   1   12    0    1    0   31    3    1   97   15    3    0    0    4    2    1   34    3    0    0 \n",
      "   1    6    4    0    0  115   39    3    3 2324    0    4    1   20    5    8   59   14    1    0 \n",
      "   3    1    2    1    2    1    0    8    3    2  125    3    0   12    0    0   12    0    0    0 \n",
      "   1    2    1   55    2    7    0   44    1    6    0  326    8    3    0    1   12    0    5    2 \n",
      "   0    1    0   40    1    1    0    0    0    3    0    7  226    0    0    0    4    2    4    3 \n",
      "   4    2    3    8    3   26    5    6    0   43    4    2    0 1983    0    1   49   15    1    0 \n",
      "   0    0    0    0    0    5    9   27    1   12    2    2    0    7  179    1    3    6    1    0 \n",
      "   0    5    1    0    4   24    4    2    7   21    0    0    0    7    0   63   23    1    0    0 \n",
      "   3   21   71    5    2   42    3   11   10   60    8    3    0   37    4    3 1177    6    1    1 \n",
      "   0    1    5    2    1   30    3    3    0   71    0    0    0   33    5    0   21  158    0    0 \n",
      "   6    0    0   25    5    1    1   22    0    2    2    2    3    8    0    0    7    0  396   32 \n",
      "   5    0    0   25    5    2    1    4    0    5    0    1    3    2    1    0    3    1   54  156 \n",
      "\n",
      "classes:\n",
      "[u'brazilian' u'british' u'cajun_creole' u'chinese' u'filipino' u'french'\n",
      " u'greek' u'indian' u'irish' u'italian' u'jamaican' u'japanese' u'korean'\n",
      " u'mexican' u'moroccan' u'russian' u'southern_us' u'spanish' u'thai'\n",
      " u'vietnamese']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import sys\n",
    "\n",
    "print \"make a confusion matrix on the best initial model\"\n",
    "cm = confusion_matrix(dev_test_label, model_logistic.predict(dev_test_vec))\n",
    "for row in cm:\n",
    "    for col in row: sys.stdout.write(\"%4d \"%col)\n",
    "    sys.stdout.write(\"\\n\")\n",
    "print\n",
    "print \"classes:\"\n",
    "print str(model_logistic.best_estimator_.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Italian and French cuisines are confusing.  Let's try putting some of the common ingredients in these two cuisines as stop words.  Also try to lemmatize the word so that we don't have several variants of the same ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7842\n"
     ]
    }
   ],
   "source": [
    "unwanted_features = ['salt', 'water', 'onion', 'garlic', 'olive', 'oil', 'clove']\n",
    "vec = TfidfVectorizer(preprocessor=text_preprocessor, ngram_range=(1,2), max_df=0.5, stop_words=unwanted_features, strip_accents='unicode', tokenizer=LemmaTokenizer())\n",
    "dev_train_vec = vec.fit_transform(dev_train_data)\n",
    "dev_test_vec = vec.transform(dev_test_data)\n",
    "# train with the best parameters on the initial model\n",
    "model_logistic2 = LogisticRegression(penalty='l2', C=11.0)\n",
    "model_logistic2.fit(dev_train_vec, dev_train_label)\n",
    "print \"accuracy: %.4f\" % model_logistic2.score(dev_test_vec, dev_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
